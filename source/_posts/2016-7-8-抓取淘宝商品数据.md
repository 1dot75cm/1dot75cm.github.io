layout: post
title: 抓取淘宝商品数据
date: 2016-07-08 00:35:21
tags: [Python, Scrapy]
categories: Python
---

昨天，看 OSChina 新闻，瞄到了 Scrapy 爬虫框架。由于之前就听说过大名，自己又空闲，因此决定尝试一下。框架的优势就是使用方便，不用自己处理 request，HTTP header 等细节。代码量较使用 urllib
等库的爬虫又少了不少。经过今天的试验，感觉挺好，因此决定尝试爬淘宝商品信息。

## 安装 Scrapy
{% codeblock lang:bash %}
$ sudo pip install Scrapy
{% endcodeblock %}

Note: 如果需要爬取 ajax，js 等动态内容，需要配合 PhantomJS，scrapy-splash 等 Headless Broswer；或使用脚本模拟 ajax 行为发送数据。

## 创建项目
{% codeblock lang:bash %}
$ scrapy startproject taobao
{% endcodeblock %}

## 编写 Spider
爬取流程：Scrapy 使用 Twisted 异步网络框架。Spider 生成 Request 由调度器执行 Downloader，爬虫继续执行。调度器完成后向 Spider 返回 Response，并调用 parse 方法解析。解析完成后，交由 Pipeline
进行后处理（进行数据过滤，保存至数据库等）。

为了简单起见，这里只编写 Spider。大致结构如下：
{% codeblock lang:python %}
class TaobaoSpider(Spider):
    # 开始请求
    def start_requests(self):
        yield Request(url, self.parse_item)

    # 解析商品内容
    def parse_item(self, response):
        data = response.body
        if '判断是否存在子类':
            for '循环子类，从最小分类解析':
                return Request(url, callback=self.parse_item)
        else:  # 不存在子类，开始解析商品
            output = '商品信息'
            filename = '输出文件名'
            with open(filename, 'a+') as f:
                f.write(output)  # 临时输出，应该由 Pipeline 处理，写入数据库
            return Request(url, callback=self.parse_item)  # 查询下一页
{% endcodeblock %}

经测试，2小时共爬取 800W 条商品信息，共 2G 数据。 完整脚本：{% link taobao_scrapy.py https://github.com/1dot75cm/repo-checker/blob/master/example/taobao_scrapy.py %}

<br/>
## 参考

- {% link Scrapy Docs http://doc.scrapy.org/en/latest/index.html %}

-- EOF --
